{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Algorithms\n",
    "This is an exercise notebook for Computer Network Performance class of 14-12-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# a map of the labels for the actions and the states \n",
    "names = {\"actions\": {0: \"maintain\", 1:\"ignore\"},\n",
    "         \"states\":{0: \"good state\", 1:\"decay\", 2:\"broken\"}}\n",
    "\n",
    "# transition probability function S x S x A\n",
    "T = [[[.9,0,.1],\n",
    "      [.9,.1,0],\n",
    "      [.2,0,.8]],\n",
    "\n",
    "      [[.5,.5,0],\n",
    "       [0,.5,.5],\n",
    "       [0,0,1]]]\n",
    "\n",
    "# rewards function S X A\n",
    "R = [[1,2],\n",
    "     [1,2],\n",
    "     [-1,0]]\n",
    "\n",
    "gamma = 0.9         # discount factor\n",
    "threshold = 10**-5  # convergence threshold \n",
    "\n",
    "n_states = len(T[0])\n",
    "n_actions = len(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(T, R, n_states, n_actions, gamma, threshold, verbose=False):\n",
    "    \"\"\"Value iteration algorithm. Given an MDP, it outputs an optimal policy.\"\"\"\n",
    "    \n",
    "    V = [0] * n_states  # optimal values \n",
    "    n = 0               # iterations count \n",
    "    while True:\n",
    "        n += 1\n",
    "        delta_epoch = 0  # how much has changed since the last epoch (triggers halt after convergence)\n",
    "        \n",
    "        for s1 in range(n_states):\n",
    "            temp = V[s1]  # old values\n",
    "            \n",
    "            # a value for s1, one for every action\n",
    "            value_actions = [ R[s1][a] + sum([T[a][s1][s2] * gamma * V[s2] for s2 in range(n_states)]) for a in range(len(T)) ]\n",
    "            V[s1] = np.max(value_actions)\n",
    "            \n",
    "            delta_epoch = max(delta_epoch, abs(temp - V[s1]))\n",
    "            \n",
    "        if verbose: \n",
    "            print(\"iter number: \", n, \"values: \", V)\n",
    "            \n",
    "        if delta_epoch < threshold:  # convergence check\n",
    "            break\n",
    "    \n",
    "    policy = [-1]*n_states\n",
    "    for s in range(n_states):\n",
    "        opt_value_actions = [ R[s][a] + sum([T[a][s][s2] * gamma * V[s2] for s2 in range(n_states)]) for a in range(len(T)) ]\n",
    "        action = np.argmax(opt_value_actions)\n",
    "        policy[s] = action\n",
    "\n",
    "    return policy\n",
    "\n",
    "def policy_to_string(plicy, names_map):\n",
    "    \"\"\" Given a policy, a list of action indicies, it returns a list of action labels. \"\"\"\n",
    "    return [names[\"actions\"][val] for val in policy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy given the MDP is:\n",
      "['ignore', 'maintain', 'maintain']\n"
     ]
    }
   ],
   "source": [
    "policy = value_iteration(T, R, n_states, n_actions, gamma, threshold, verbose=False)\n",
    "policy_readable = policy_to_string(policy, names)\n",
    "print(\"Optimal policy given the MDP is:\\n\" + str(policy_readable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(T, R, n_states, n_actions, gamma, threshold, verbose=False):\n",
    "    \"\"\"Policy iteration algorithm. Given an MDP, it outputs an optimal policy.\"\"\"\n",
    "    \n",
    "    V = [0] * n_states       # optimal values \n",
    "    policy = [0] * n_states  # optimal policy\n",
    "    n = 0\n",
    "\n",
    "    while True:\n",
    "        while True:\n",
    "            n += 1\n",
    "            delta_epoch = 0 # how much the values have changed since the last epoch (triggers halt after convergence)\n",
    "            \n",
    "            for s1 in range(n_states):\n",
    "                temp = V[s1]\n",
    "                a = policy[s1] # action of the policy under evaluation\n",
    "                V[s1] = R[s1][a] + sum([T[a][s1][s2] * gamma * V[s2] for s2 in range(n_states)])\n",
    "                \n",
    "                delta_epoch = max(delta_epoch, abs(temp - V[s1]))\n",
    "\n",
    "            if verbose: \n",
    "                print(\"iter number: \", n, \"values: \", V)\n",
    "                \n",
    "            if delta_epoch < threshold:  # values convergence check\n",
    "                break\n",
    "\n",
    "        is_stable = True  # if the policy changed since the last epoch (triggers halt after convergence)\n",
    "        for s in range(n_states):\n",
    "            temp = policy[s]\n",
    "            # a value for s, one for every action\n",
    "            vals = [ R[s][a] + sum([T[a][s][s2] * gamma * V[s2] for s2 in range(n_states)]) for a in range(len(T)) ]\n",
    "            action = np.argmax(vals)\n",
    "            policy[s] = action\n",
    "\n",
    "            if temp != policy[s]:\n",
    "                is_stable = False\n",
    "\n",
    "        if is_stable == True:  # policy convergence check\n",
    "            break\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy given the MDP is:\n",
      "['ignore', 'maintain', 'maintain']\n"
     ]
    }
   ],
   "source": [
    "policy = policy_iteration(T, R, n_states, n_actions, gamma, threshold, verbose=False)\n",
    "policy_readable = policy_to_string(policy, names)\n",
    "print(\"Optimal policy given the MDP is:\\n\" + str(policy_readable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a map of the labels for the actions and the states \n",
    "names = {\"actions\": {0:\"go 0\", 1:\"go 1\", 2:\"go 2\", 3:\"go 3\", 4:\"go 4\", 5:\"go 5\"},\n",
    "         \"states\":{0:\"room 0\", 1:\"room 1\", 2:\"room 2\", 3:\"room 3\", 4:\"room 4\", 5:\"OUT\"}}\n",
    "\n",
    "# transition probability function S x S x A\n",
    "# T = IS UNKNOWN\n",
    "\n",
    "# rewards function S X A\n",
    "R = [[-1,-1,-1,-1,0,-1],\n",
    "     [-1,-1,-1,0,-1,100],\n",
    "     [-1,-1,-1,0,-1,-1],\n",
    "     [-1,0,0,-1,0,-1],\n",
    "     [0,-1,-1,0,-1,100],\n",
    "     [-1,0,-1,-1,0,100]]\n",
    "\n",
    "R = np.array(R)  # let is be a nupy array for convenience\n",
    "\n",
    "gamma = 0.9         # discount factor\n",
    "threshold = 10**-5  # convergence threshold \n",
    "alfa = 0.8           # learning rate\n",
    "epsilon = .9999        # probability (decaying) to get a random action\n",
    "\n",
    "n_states = len(R)\n",
    "n_actions = len(R[0])\n",
    "\n",
    "end_state = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def qlearning(R, end_state, n_states, n_actions, gamma, alfa, threshold, epsilon, seed=0, verbose=False):\n",
    "    \"\"\"Policy iteration algorithm. Given an MDP, it outputs an optimal policy.\"\"\"\n",
    "    \n",
    "    np.random.seed(seed) # to set randomness\n",
    "    \n",
    "    Q = np.array([[0]* n_states] * n_states)    # optimal q values \n",
    "    policy = [0] * n_states                     # optimal policy\n",
    "    stable_q = 0\n",
    "\n",
    "    while True:\n",
    "        s1 = np.random.randint(n_states)  # choose a random state to start this epoch\n",
    "        s2 = -1                           # the target state is still unknown, need to choose and action first\n",
    "        temp = Q\n",
    "        reached_end_epoch = False \n",
    "        \n",
    "        while not reached_end_epoch: # stop if the node was an ending node\n",
    "            \n",
    "            if s2 == end_state:\n",
    "                reached_end_epoch = True\n",
    "            \n",
    "            available_actions_s1 = np.where(R[s1]>=0)[0] # indicies of non null actions from s1\n",
    "            \n",
    "            # epsilon probability gets smaller with time, encourage exploitation rather than exploration \n",
    "            if flip_bised_coin(epsilon): \n",
    "                # explore\n",
    "                a = np.random.choice(available_actions_s1, 1)[0]  # actions randomly selected\n",
    "                epsilon = epsilon * epsilon                       # exp decay probability of random action\n",
    "            else:\n",
    "                # exploit\n",
    "                if (Q[s1] == 0).all():                                #Â all zeros, then chose available action\n",
    "                    a = np.random.choice(available_actions_s1, 1)[0]  # actions randomly selected\n",
    "                else:\n",
    "                    a = np.argmax(Q[s1]) \n",
    "            \n",
    "            #time.sleep(1)\n",
    "            s2 = a        # a is both the action and the future state!\n",
    "            r = R[s1][a]  # reward from having executed a from s1 and having reached state s2\n",
    "            \n",
    "            # update the qtable\n",
    "            Q[s1][a] = (1-alfa) * Q[s1][a] + alfa * (r + gamma * np.max(Q[s2]))\n",
    "            s1 = s2  # restart the process from s2 now\n",
    "        \n",
    "        \n",
    "        #### convergence check ####\n",
    "        converged = (np.abs(temp - Q) < threshold).all()\n",
    "\n",
    "        if converged:\n",
    "            stable_q += 1\n",
    "            if stable_q > 100:  # the q table was stable for 20 consecutive epochs\n",
    "                if verbose: \n",
    "                    print(\"Qtable:\\n\" + str(Q) + \"\\n\")\n",
    "                break \n",
    "        else:\n",
    "            stable_q = 0            \n",
    "                    \n",
    "    # reconstruct policy \n",
    "    for s in range(n_states):\n",
    "        policy[s] = np.argmax(Q[s])\n",
    "            \n",
    "    return policy\n",
    "\n",
    "def flip_bised_coin(p):\n",
    "    return True if np.random.random() < p else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable:\n",
      "[[  0   0   0   0 888   0]\n",
      " [  0   0   0   0   0 988]\n",
      " [  0   0   0 798   0   0]\n",
      " [  0   0   0   0 888   0]\n",
      " [  0   0   0  41   0 988]\n",
      " [  0   0   0   0 475 988]]\n",
      "\n",
      "Optimal policy given the Q learning problem is:\n",
      "['go 4', 'go 5', 'go 3', 'go 4', 'go 5', 'go 5']\n"
     ]
    }
   ],
   "source": [
    "policy = qlearning(R, end_state, n_states, n_actions, gamma, alfa, threshold, epsilon, verbose=True)\n",
    "policy_readable = policy_to_string(policy, names)\n",
    "print(\"Optimal policy given the Q learning problem is:\\n\" + str(policy_readable))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
