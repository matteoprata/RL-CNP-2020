{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Algorithms\n",
    "This is an exercise notebook for Computer Network Performance class of 14-12-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/car.png\" width=\"1000\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# a map of the labels for the actions and the states \n",
    "names = {\"actions\": {0: \"maintain\", 1:\"ignore\"},\n",
    "         \"states\":{0: \"good state\", 1:\"decay\", 2:\"broken\"}}\n",
    "\n",
    "# transition probability function S x S x A\n",
    "T = [[[.9,0,.1],\n",
    "      [.9,.1,0],\n",
    "      [.2,0,.8]],\n",
    "\n",
    "      [[.5,.5,0],\n",
    "       [0,.5,.5],\n",
    "       [0,0,1]]]\n",
    "\n",
    "# rewards function S X A\n",
    "R = [[1,2],\n",
    "     [1,2],\n",
    "     [-1,0]]\n",
    "\n",
    "gamma = 0.9         # discount factor\n",
    "threshold = 10**-5  # convergence threshold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/value_function.png\" width=\"1000\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(T, R, gamma, threshold, verbose=False):\n",
    "    \"\"\"Value iteration algorithm. Given an MDP, it outputs an optimal policy.\"\"\"\n",
    "    \n",
    "    n_states = len(T[0])\n",
    "    n_actions = len(T)\n",
    "\n",
    "    V = [0] * n_states  # optimal values \n",
    "    n = 0               # iterations count \n",
    "    while True:\n",
    "        n += 1\n",
    "        delta_epoch = 0  # how much has changed since the last epoch (triggers halt after convergence)\n",
    "\n",
    "        for s_src in range(n_states):\n",
    "            temp = V[s_src]  # old values\n",
    "            \n",
    "            # a value for s_src, one for every action\n",
    "            value_actions = [ R[s_src][a] + sum([T[a][s_src][s_dst] * gamma * V[s_dst] \n",
    "                                              for s_dst in range(n_states)]) \n",
    "                                                  for a in range(n_actions) ]\n",
    "            V[s_src] = np.max(value_actions)\n",
    "            delta_epoch = max(delta_epoch, abs(temp - V[s_src]))\n",
    "            \n",
    "        if verbose: \n",
    "            print(\"iter number: \", n, \"values: \", V)\n",
    "            \n",
    "        if delta_epoch < threshold:  # convergence check\n",
    "            break\n",
    "    \n",
    "    policy = [-1]*n_states\n",
    "    for s_src in range(n_states):\n",
    "        opt_value_actions = [ R[s_src][a] + sum([T[a][s_src][s_dst] * gamma * V[s_dst] \n",
    "                                             for s_dst in range(n_states)])\n",
    "                                                 for a in range(n_actions) ]\n",
    "        action = np.argmax(opt_value_actions)\n",
    "        policy[s_src] = action\n",
    "\n",
    "    return policy\n",
    "\n",
    "def policy_to_string(plicy, names_map):\n",
    "    \"\"\" Given a policy, a list of action indicies, it returns a list of action labels. \"\"\"\n",
    "    return [names[\"actions\"][val] for val in policy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy for the given MDP is:\n",
      "['ignore', 'maintain', 'maintain']\n"
     ]
    }
   ],
   "source": [
    "policy = value_iteration(T, R, gamma, threshold, verbose=False)\n",
    "policy_readable = policy_to_string(policy, names)\n",
    "print(\"Optimal policy for the given MDP is:\\n\" + str(policy_readable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(T, R, gamma, threshold, verbose=False):\n",
    "    \"\"\"Policy iteration algorithm. Given an MDP, it outputs an optimal policy.\"\"\"\n",
    "    \n",
    "    n_states = len(T[0])\n",
    "    n_actions = len(T)\n",
    "    \n",
    "    V = [0] * n_states       # optimal values \n",
    "    policy = [0] * n_states  # optimal policy\n",
    "    n = 0\n",
    "\n",
    "    while True:\n",
    "        while True:\n",
    "            n += 1\n",
    "            delta_epoch = 0 # how much the values have changed since the last epoch (triggers halt after convergence)\n",
    "            \n",
    "            for s_src in range(n_states):\n",
    "                temp = V[s_src]\n",
    "                a = policy[s_src] # action of the policy under evaluation\n",
    "                V[s_src] = R[s_src][a] + sum([T[a][s_src][s_dst] * gamma * V[s_dst] for s_dst in range(n_states)])\n",
    "                \n",
    "                delta_epoch = max(delta_epoch, abs(temp - V[s_src]))\n",
    "\n",
    "            if verbose: \n",
    "                print(\"iter number: \", n, \"values: \", V)\n",
    "                \n",
    "            if delta_epoch < threshold:  # values convergence check\n",
    "                break\n",
    "\n",
    "        is_stable = True  # if the policy changed since the last epoch (triggers halt after convergence)\n",
    "        for s_src in range(n_states):\n",
    "            temp = policy[s_src]\n",
    "            # a value for s, one for every action\n",
    "            vals = [ R[s_src][a] + sum([T[a][s_src][s_dst] * gamma * V[s_dst] \n",
    "                                    for s_dst in range(n_states)]) \n",
    "                                        for a in range(n_actions) ]\n",
    "            action = np.argmax(vals)\n",
    "            policy[s_src] = action\n",
    "\n",
    "            if temp != policy[s_src]:\n",
    "                is_stable = False\n",
    "\n",
    "        if is_stable == True:  # policy convergence check\n",
    "            break\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy for the given MDP is:\n",
      "['ignore', 'maintain', 'maintain']\n"
     ]
    }
   ],
   "source": [
    "policy = policy_iteration(T, R, gamma, threshold, verbose=False)\n",
    "policy_readable = policy_to_string(policy, names)\n",
    "print(\"Optimal policy for the given MDP is:\\n\" + str(policy_readable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/room.png\" width=\"1000\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a map of the labels for the actions and the states \n",
    "names = {\"actions\": {0:\"go 0\", 1:\"go 1\", 2:\"go 2\", 3:\"go 3\", 4:\"go 4\", 5:\"go OUT\"},\n",
    "         \"states\":{0:\"room 0\", 1:\"room 1\", 2:\"room 2\", 3:\"room 3\", 4:\"room 4\", 5:\"OUT\"}}\n",
    "\n",
    "# rewards function S X A\n",
    "R = [[-1,-1,-1,-1,0,-1],\n",
    "     [-1,-1,-1,0,-1,100],\n",
    "     [-1,-1,-1,0,-1,-1],\n",
    "     [-1,0,0,-1,0,-1],\n",
    "     [0,-1,-1,0,-1,100],\n",
    "     [-1,0,-1,-1,0,100]]\n",
    "\n",
    "R = np.array(R)  # let is be a numpy array for convenience\n",
    "\n",
    "gamma = 0.8         # discount factor\n",
    "threshold = 10**-5  # convergence threshold \n",
    "alfa = 1            # learning rate\n",
    "\n",
    "epsilon = 1        # probability (decaying) to get a random action\n",
    "decay = 0.0005      \n",
    "\n",
    "end_state = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/ql.png\" width=\"1000\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning(R, end_state, gamma, alfa, threshold, epsilon, decay, seed=0, verbose=False):\n",
    "    \"\"\"Policy iteration algorithm. Given an MDP, it outputs an optimal policy.\"\"\"\n",
    "    \n",
    "    n_states = len(R)\n",
    "    n_actions = len(R[0])\n",
    "\n",
    "    np.random.seed(seed)                        # to set randomness\n",
    "    Q = np.array([[0]* n_states] * n_states)    # optimal q values \n",
    "\n",
    "    while True:\n",
    "        s_src = np.random.randint(n_states)  # choose a random state to start this epoch\n",
    "        s_dst = None                         # the target state is still unknown, need to choose and action first\n",
    "\n",
    "        reached_end_epoch = False \n",
    "        delta_epoch = 0\n",
    "        while not reached_end_epoch: # stop if the node was an ending node\n",
    "            \n",
    "            if s_dst == end_state:\n",
    "                reached_end_epoch = True\n",
    "            \n",
    "            available_actions_s_src = np.where(R[s_src]>=0)[0] # indicies of non null actions from s1\n",
    "            \n",
    "            # epsilon probability gets smaller with time, encourage exploitation rather than exploration \n",
    "            if flip_bised_coin(epsilon): \n",
    "                # explore\n",
    "                a = np.random.choice(available_actions_s_src, 1)[0]  # actions randomly selected\n",
    "            else:\n",
    "                # exploit\n",
    "                #Â its the action with the max Q from s1 if there are positive Q, otherwise a random available action  \n",
    "                a = np.argmax(Q[s_src]) if not (Q[s_src] == 0).all() else np.random.choice(available_actions_s_src, 1)[0]\n",
    "            \n",
    "            epsilon = epsilon - decay # decay probability of random action\n",
    "            \n",
    "            s_dst = a           # a is both the action and the future state\n",
    "            r = R[s_src][a]     # reward from having executed a from s1 \n",
    "            temp = Q[s_src][a]  # store the old Q value for comparison on convergence check\n",
    "            \n",
    "            Q[s_src][a] = (1-alfa) * Q[s_src][a] + alfa * (r + gamma * np.max(Q[s_dst])) # update the qtable               \n",
    "        \n",
    "            delta_epoch = max(delta_epoch, abs(temp - Q[s_src][a]))\n",
    "            s_src = s_dst  # let the process evolve from s2 now\n",
    "\n",
    "\n",
    "        # convergence check \n",
    "        if delta_epoch < threshold and epsilon <= 0:\n",
    "            if verbose: \n",
    "                print(\"Qtable:\\n\" + str(Q) + \"\\n\")\n",
    "            break            \n",
    "                    \n",
    "    # reconstruct the policy \n",
    "    policy = [0] * n_states  # optimal policy\n",
    "    \n",
    "    for s_src in range(n_states):\n",
    "        policy[s_src] = np.argmax(Q[s_src])\n",
    "            \n",
    "    return policy\n",
    "\n",
    "def flip_bised_coin(p):\n",
    "    \"\"\" Return true with probability p, false with probability 1-p \"\"\"\n",
    "    return np.random.random() < p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable:\n",
      "[[  0   0   0   0 396   0]\n",
      " [  0   0   0 316   0 496]\n",
      " [  0   0   0 316   0   0]\n",
      " [  0 396 252   0 396   0]\n",
      " [316   0   0 316   0 496]\n",
      " [  0 396   0   0 396 496]]\n",
      "\n",
      "Optimal policy is:\n",
      "['go 4', 'go OUT', 'go 3', 'go 1', 'go OUT', 'go OUT']\n"
     ]
    }
   ],
   "source": [
    "policy = qlearning(R, end_state, gamma, alfa, threshold, epsilon, decay, verbose=True)\n",
    "policy_readable = policy_to_string(policy, names)\n",
    "print(\"Optimal policy is:\\n\" + str(policy_readable))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
